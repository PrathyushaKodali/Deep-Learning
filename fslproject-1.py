# -*- coding: utf-8 -*-
"""FSLProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Kjbs5oXFNZvE8NVpotXT_C3fw7KWXG-m
"""

import numpy as np
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
import sklearn
from statistics import mean

#%cd sample_data/

def load_data():
  
  # Load train files
  df1 = pd.read_csv('Train1.txt', sep="  ", header=None)
  train_1 = df1[:1500] 
  val_1 = df1[1500:]

  df2 = pd.read_csv('Train2.txt', sep="  ", header=None)
  train_2 = df2[:1500] 
  val_2 = df2[1500:]

  X_train = pd.concat([train_1,train_2])
  X_val = pd.concat([val_1,val_2])

  X_train = X_train.reset_index(drop=True)
  X_val = X_val.reset_index(drop=True)

  #  Create labels.  Setting class 1 label to 0 and class 2 label to 1
  labels_0 = np.zeros(df1.shape[0])
  labels_1 = np.ones(df2.shape[0])

  y_train_data = np.concatenate((labels_0[:1500],labels_1[:1500]))
  y_val_data = np.concatenate((labels_0[1500:],labels_1[1500:]))
  
  y_train=pd.DataFrame()
  y_val = pd.DataFrame()
  y_train['Labels'] = y_train_data
  y_val['Labels'] = y_val_data
  
  y_train = y_train.reset_index(drop=True)
  y_val = y_val.reset_index(drop=True)

  return X_train,X_val,y_train,y_val

def shuffle(X,y):
  Set = X.join(y)
  df_set = Set.sample(frac = 1).reset_index(drop=True)

  X_df = pd.DataFrame(df_set.iloc[:,:-1])
  y_df = pd.DataFrame(data=df_set.iloc[:,-1]).rename(columns = {'Labels': 0}, inplace = False)
  
  return X_df,y_df

def load_test_data():
  # Load test files
  df1 = pd.read_csv('Test1.txt', sep="  ", header=None)
  df2 = pd.read_csv('Test2.txt', sep="  ", header=None)

  X_test = pd.concat([df1,df2])
  X_test = X_test.reset_index(drop=True)

  #  Create labels.  Setting class 1 label to 0 and class 2 label to 1
  labels_0 = np.zeros(df1.shape[0])
  labels_1 = np.ones(df2.shape[0])

  y_test_data = np.concatenate((labels_0,labels_1))

  y_test=pd.DataFrame()
  y_test['Labels'] = y_test_data
  y_test = y_test.reset_index(drop=True)

  return X_test,y_test

def normalize(df):
    normalized_df=(df-df.mean())/df.std()
    return normalized_df

def initialize_parameters(input_dim,hidden_dim,output_dim):
    
    # Input-hidden weights
    W1 = np.random.randn(input_dim, hidden_dim) * 2 - 1
    
    # Input - hidden bias
    b1 = np.zeros((1, hidden_dim))
    
    # Hidden - Output weights
    W2 = np.random.randn(hidden_dim, output_dim)* 2 - 1
    
    # Hidden - Output bias
    b2= np.zeros((1, output_dim))
    
    # Return model parameters
    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}

    return model

def sigmoid(z):
  return 1/(1 + np.exp(-z))

def sigmoid_derivative(z):
  return np.multiply(sigmoid(z),1-sigmoid(z))

def forward_propagation(model,X):        
  
  # Retriev parameters from model    
  W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']

  # Linear transformation from input-hidden layer  
  z1 = X.dot(W1) + b1 
  #print("Z1",z1)
  
  # Applying sigmoid activation function     
  a1 = sigmoid(z1)
  #print("a1",a1)

  # Linear transformation from hidden-output layer   
  z2 = a1.dot(W2) + b2    

  # Applying sigmoid activation function     
  a2 = sigmoid(z2)
  #print("a2",a2)

  #Storing all results in cache    
  cache = {'a0':X,'z1':z1,'a1':a1,'z2':z2,'a2':a2}    
  return cache

def calculate_MSE(Yhat, Y):   
    m = Y.shape[0] 
    SE = np.square(np.subtract(Y,Yhat))   
    error = (1/m)*np.sum(SE)
    err = np.array(error)  
    return err[0]

def backward_propagation(y,model,cache):
  
  # Load parameters from model    
  W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2'] 

  # Load forward propagation values stored in cache    
  a0, a1, a2 = cache['a0'],cache['a1'],cache['a2']  
  z2 = cache['z2']
  z1 = cache['z1']
  # Get number of samples    
  m = y.shape[0] 

  # Calculate dW2 = dJ/dW2 = dJ/dnet(k) *dnet(k)/dW2
  dz2 = a2-y
  delta = np.multiply(dz2,sigmoid_derivative(z2))
  
  # Calculate loss derivative with respect to hidden layer weights    
  dW2 = (1/m)*np.dot(a1.T,delta)
  #print("W2",W2)
  #print("dW2",dW2)
      
  # Calculate loss derivative with respect to hidden layer bias    
  db2 = 1/m*np.sum(np.array(dz2), axis=0,keepdims=True)
  #print("b2",b2)
  #print("db2",db2)   

  # Calculate loss derivative with respect to hidden layer   
  dz1 = np.dot(delta,W2.T)

  dW1 = 1/m*np.dot(a0.T,sigmoid_derivative(z1))*np.sum(dz1)  
  #print("W1",W1)
  #print("dw1",dW1)
       
  db1 = 1/m*np.sum(np.array(dz1),axis=0,keepdims=True)
  #print("b1",b1)
  #print("db1",db1) 

  # Store gradients    
  grads = {'dW2':dW2,'db2':db2,'dW1':dW1,'db1':db1}    
  return grads

def update_parameters(grads_prev,momentum,model,grads,learning_rate):
    # Load parameters
    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']
    
    # Update parameters
    if grads_prev==None:
      W1 -= learning_rate * grads['dW1']
      b1 -= learning_rate * grads['db1']
      W2 -= learning_rate * grads['dW2']
      b2 -= learning_rate * grads['db2']
    else:
      W1 -= ((learning_rate * grads['dW1']) + (momentum*grads_prev['dW1']))
      b1 -= ((learning_rate * grads['db1']) + (momentum*grads_prev['db1']))
      W2 -= ((learning_rate * grads['dW2']) + (momentum*grads_prev['dW2']))
      b2 -= ((learning_rate * grads['db2']) + (momentum*grads_prev['db2']))

    
    # Store and return parameters
    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}
    return model

def predict(model, x):
    # Do forward pass
    c = forward_propagation(model,x)
    return c['a2']

def calc_accuracy(model,pred,y):
    m = y.shape[0]
    error = np.sum(np.abs(pred-y))
    err = np.array(error)  
    return (m - err[0])/m * 100

def train(model,X,y, X_val,y_val,X_test,y_test,learning_rate, epochs=20000):
    
    J_prev = 9999999999999
    terminate = False # set the initial value to False
    i =0
    
    loss_train=[]
    loss_val =[]
    loss_test =[]
    test_accuracy = []

    # Gradient descent. Loop until this terminate flag is turned on when J of validation set no longer decreases
    while terminate == False and i<epochs:

        # Forward propagation
        cache = forward_propagation(model,X)
        y_hat = cache['a2']

        # Back propagation
        grads = backward_propagation(y,model,cache)
        if i==0:
          grads_prev = None

        # update the weights
        momentum = 0.05
        model = update_parameters(grads_prev,momentum,model=model,grads=grads,learning_rate=learning_rate)
        grads_prev=grads

        # Compute MSE for training set, validation set and test set
        J_train = calculate_MSE(y_hat, y)
        y_hat_val = predict(model,X_val)
        J_val = calculate_MSE(y_hat_val, y_val)
        y_hat_test = predict(model,X_test)
        J_test = calculate_MSE(y_hat_test, y_test) 
        
        loss_train.append(J_train)
        loss_val.append(J_val)
        loss_test.append(J_test)

        #Calculate accuracy score of test sets
        accuracy_score = calc_accuracy(model,y_hat_test,y)
        test_accuracy.append(accuracy_score)

        # Check for stopping criteria
        stop_threshold = 0.02
        if J_val>J_prev+stop_threshold:
         terminate = True
        J_prev = J_val 

        i+=1;

    print("training ended at epoch",i)
    return model,mean(test_accuracy),J_val,loss_train,loss_val,loss_test

# ******************************************************************************************#
                   ##  Step 1: Load datasets  ##
#*******************************************************************************************#
# Load train datasets
X_train_data,X_valid_data,y_train_data,y_valid_data = load_data()

# Load test datasets  
X_test_data,y_test_data = load_test_data()

#**********************************************************************************************#
               ## Step 2: TRAINING and TESTING
#**********************************************************************************************#
Accuracy = []

for hidden_units in range(2,11,2):
  print("------------------------------ Training model with ",hidden_units," hidden units ----------------------------")

  #Shuffle the train_test, validation sets, test sets
  X_train, y_train = shuffle(X_train_data,y_train_data)
  X_val, y_val = shuffle(X_valid_data,y_valid_data)
  X_test, y_test = shuffle(X_test_data,y_test_data)

  #Normalize data 
  X_norm = normalize(X_train) 
  X_norm_val = normalize(X_val)
  X_norm_test = normalize(X_test)

  # Initialize parameters
  MLP_model = initialize_parameters(2, hidden_units, 1)

  # train and test the model
  epochs=4000
  learning_rate=0.7
  model, avg_test_accuracy,J_stop,loss_train, loss_val, loss_test = train(MLP_model,X_norm,y_train,X_norm_val,y_val,X_norm_test,y_test,learning_rate,epochs)
  
  print("Test accuracy",avg_test_accuracy)
  print("Validation error at which the training ends",J_stop)
  print("Minimum validation error occured at epoch",loss_val.index(min(loss_val)))
  print("Minimum test error occured at epoch",loss_test.index(min(loss_test)))
  print("Minimum train error occured at epoch",loss_train.index(min(loss_train)))

  Accuracy.append(avg_test_accuracy)

#**********************************************************************************************#
               ## Step 3:  Plotting the learning curve ## 
#**********************************************************************************************#
  no_of_vals = len(loss_train)
  epochs = list(range(0, no_of_vals))
  plt.plot(epochs, loss_train, label='training')
  plt.plot(epochs, loss_val, label='validation')
  plt.plot(epochs, loss_test, label='test')
  plt.legend()
  plt.show()

#**********************************************************************************************#
               ## Step 4:  Reporting best classification test accuracy ## 
#**********************************************************************************************#
maxpos = Accuracy.index(max(Accuracy)) 
print("Best Classification Accuracy for the testing set is reported when number of hidden units are ",(maxpos+1)*2)